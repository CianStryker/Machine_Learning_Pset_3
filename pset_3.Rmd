---
title: "Problem Set 3"
author: "Cian Stryker"
date: "10/16/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(AER)
library(fastDummies)
library(ISLR)
library(pls)
library(tree)
library(ggthemes)
library(janitor)

```


# Concept Questions

#### Question 1

\hfill\break

You should standardize your variables for Ridge, Lasso, KNN, PCR, and PLS. You do not need to standardize your variables when doing a logistic or linear regression. 

#### Question 2

\hfill\break

We cannot conclude that we should choose Model B. This is because model B's AUC (Area Under The Curve) is higher and, generally speaking, while a higher AUC indicates a model is better at predicting, it also has a higher chance for false positives. Model A has a lower AUC than Model B, which means it is further down on the ROC curve. This indicates that model A will likely produce fewer false positives than model B. We should therefore choose Model A because while it is worse at prediction overall, our primary goal is only to minimize the number of false positives and model A is better suited to this goal.  

#### Question 3

\hfill\break

If we run a Lasso with a lambda of 0 the model is equivalent to an OLS regression. As lambda approaches infinity, however, the shrinkage penalty grows and all coefficients will be reduced to zero, removing them from the model itself. 

#### Question 4

\hfill\break

When the number of principal components in a model is high and close to the number of features in the data set, Ridge and Lasso regressions are likely to outperform PCR. 


#### Question 5

\hfill\break

Yes I would conclude that X2 will receive more weight in the PLS regression because it is more highly correlated to our outcome variable (Y). PLS takes the correlation of predictors to the outcome variable into account when allocating weight, therefore, x2's higher correlation to Y would indicate it will receive more weight.  

#### Question 6

\hfill\break

No I would not necessarily conclude that X2 will receive more weight in the PCR regression. PCR does not take the relationship of predictors to the outcome variable (Y) into account when allocating weight, therefore, even though  X2 has a higher correlation to Y, PCR will not take that into account when allocating weight. 

#### Question 7

\hfill\break

A polynomial non-spline regression imposes a global structure to data that doesn't necessarily take into account the differences between certain segments of the data range. A step function might be more appropriate for this situation because it divides the range of data into different bins or k distinct regions, each with a different constant. In the situation given, we could create a bin that shows years 10-11 and 11-12 separately which may show if that 12th year really does have a significant impact. 


#### Question 8

\hfill\break


When we determine how many knots or degrees of freedom to use for a regression spline we should use cross validation to try different degrees of freedom or knots (k values) and then choose the number with the lowest RSS.

#### Question 9

\hfill\break

Natural regression splines add additional constraints at the tails of a model, which decrease the variance at the tails. In polynomial regressions the variance at the tails is often very high because data is limited. Natural regression splines address this issue and produces more stable estimates at the boundaries.  

#### Question 10

\hfill\break

For smoothing splines, if the smoothing parameter is 0 the function will be very rough, but if the parameter is equal to infinite it will become perfectly smooth. 

# Data Questions

#### 1.) 

\hfill\break

```{r Data Loading and q1, echo=FALSE}

data("STAR")

data_1 <- STAR %>%
  select(-c(birth, readk, read1, read2, mathk, math1, math2))

q1 <- ncol(data_1)

data_2 <- data_1 %>%
  drop_na()

first <- data_1 %>%
  nrow()

second <- data_2 %>%
  nrow()

q2 <- first - second

# sapply(data_2, class)

for (i in 40:1) {
  if (is.factor(data_2[,i])) {
    for (j in unique(data_2[,i])) {
      new_col             <- paste(colnames(data_2)[i], j, sep = "_")
      data_2[,new_col] <- as.numeric(data_2[,i] == j) 
    }
    data_2       <- data_2[,-i]     
  } else if (typeof(data_2[,i]) == "integer") {
    data_2[,i]   <- as.numeric(as.character(data_2[,i]))
  } 
}

q4_predictors <- ncol(data_2)
q4_observations <- nrow(data_2)

# for (col_num in 1:ncol(data_2)) {
#   if (var(data_2[,col_num]) < 0.05) {
#     print(colnames(data_2)[col_num])
#     print(var(data_2[,col_num]))
#   }
# }

for (col_num in ncol(data_2):1) {
  if (var(data_2[,col_num]) < 0.05) {
    data_2       <- data_2[,-col_num]
  }
}

data_2 <- data_2 %>%
  clean_names()

q4_predictors2 <- ncol(data_2)

q4 <- q4_predictors - q4_predictors2

set.seed(13194)

train               <- sample(seq(nrow(data_2)),
                              floor(nrow(data_2) * 0.8))

train               <- sort(train)

test                <- which(!(seq(nrow(data_2)) %in% train))
```

a.) There are 40 predictors in the data set. 

b.) 9230 observations were missing values. 

c.) 34 variables are categorical variables, while six are numeric. 

d.) 462 variables have variance less than 0.05. 



```{r Question 2, echo=FALSE}

set.seed(13194)


pcr_fit <- pcr(read3~., data = data_2[train,], 
               scale = TRUE, 
               validation = "CV", 
               nfold = 10)
# summary(pcr_fit)

# 36.91-34.64

# 34.64-34.40
```
#### 2.) 

\hfill\break

a.) I would not say that the model with 10 principal components shows a big improvement in CV RMSE over the model with 0 components because the difference between the two is only 2.27

b.) The model with 20 principal components has a slightly lower CV RMSE than the model with 10, but only 0.24 lower, which is a very small improvement.   

c.) 0 components has a CV RMSE of 36.91, 10 components has a CV RMSE of 34.64, and 20 components has a CV RMSE of 34.30. 

```{r Question 3, echo=FALSE}


pcr_msep <- MSEP(pcr_fit)
pcr_min_indx <- which.min(pcr_msep$val[1,1,])
# print(pcr_min_indx)

q3_1 <- pcr_msep$val[1,1,pcr_min_indx]

q3_b <- sqrt(q3_1)

pcr_pred <- predict(pcr_fit, data_2[test,],
                    ncomp = 39)

pcr_test_MSE <- mean((pcr_pred - data_2[test,"read3"])^2)

# print(pcr_test_MSE)

q3_c <- (sqrt(pcr_test_MSE))
```

#### 3.) 

\hfill\break

a.) 39 component principled components corresponds to the lowest CV RMSE. 

b.) The CV RMSE of 39 component parts is 24.944

c.) The test RMSE for the test data and 38 components is 25.890. 

```{r Question 4,echo=FALSE}

set.seed(13194)


pls_fit <- plsr(read3~., data = data_2[train,], 
               scale = TRUE, 
               validation = "CV", 
               nfold = 10)


pls_msep <- MSEP(pls_fit)
pls_min_indx <- which.min(pls_msep$val[1,1,])
# print(pls_min_indx)

q4_1 <- pls_msep$val[1,1,pls_min_indx]

q4_b <- sqrt(q4_1)

pls_pred          <- predict(pls_fit, data_2[test,],
                             ncomp = 7)

pls_test_MSE      <- mean((pls_pred - data_2[test,"read3"])^2)

# print(pls_test_MSE)

q4_c <- sqrt(pls_test_MSE)

```

#### 4.) 

\hfill\break

a.) 7 component principled components corresponds to the lowest CV RMSE. 

b.) The CV RMSE of 7 component parts is 24.958

c.) The test RMSE for the test data and 7 components is 25.853. 

```{r Question 5, echo=FALSE}

set.seed(13194)

tree_model <- tree(read3~., data = data_2, subset = train)
# summary(tree_model)

cv_tree_model <- cv.tree(tree_model)

best_indx <- which.min(cv_tree_model$dev)
best_size <- cv_tree_model$size[best_indx]
```

#### 5.) 

\hfill\break

a.) The optimal size is 5. 

b.) 


```{r Question 5 part b}

plot_data <- data.frame(size = cv_tree_model$size, deviance = cv_tree_model$dev)

ggplot(plot_data, aes(x = size, y = deviance)) +
  geom_point() +
  geom_line() +
  labs(
    title = "Deviance Compared to Size",
    x = "Size",
    y = "Deviance"
  ) +
  theme_classic()

```

```{r Question 5 part c, echo=FALSE}
tree_preds <- predict(tree_model, 
                      newdata = data_2[test,])

msep_func <- function(predictions, true_vals) {
  MSEP    <- mean((predictions - true_vals)^2)
  return(MSEP)
}

q5_c1 <- msep_func(tree_preds, data_2[test, "read3"])

q5_c <- sqrt(q5_c1)

```

c.) The test RMSE is 26.309


#### 6.) 

\hfill\break

If I had to predict third grade reading scores using one of the methods tried in this problem set, I would choose PLS over PCR or decision trees because its test RMSE was the lowest of the three models. 
